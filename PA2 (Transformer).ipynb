{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4cd103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Roi Yehoshua\n",
    "# Date: January 2024\n",
    "# MIT License \n",
    "\n",
    "# Based on the PyTorch implementation from https://nlp.seas.harvard.edu/annotated-transformer/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6bbb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure you have the following packages installed: \n",
    "!pip install spacy torchtext portalocker --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a5dee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import math\n",
    "import spacy\n",
    "import os\n",
    "\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.datasets import Multi30k\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17027ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)  # For reproducibility\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffad93f9",
   "metadata": {},
   "source": [
    "### Multi-Head Attention\n",
    "\n",
    "$$\n",
    "    \\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\ldots, \\text{head}_h)W^O \\\\\n",
    "    \\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V) \\\\  \n",
    "    \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf156177",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"The multi-head attention module\"\"\"\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__() # calls the constructor of the parent class: import torch.nn as nn; a custom neural network module that inherits from nn.Module in PyTorch\n",
    "        \n",
    "        # Ensure the dimension of the model is divisible by the number of heads.\n",
    "        # This is necessary to equally divide the embedding dimension across heads.\n",
    "        assert d_model % num_heads == 0, 'd_model must be divisible by num_heads'\n",
    "        \n",
    "        self.d_model = d_model     # assigns the value from the parameter d_model to the instance variable self.d_model to store the value later use in other methods of the clas.  # Total dimension of the model, d_model is the size of the embedding vector for each token in the sequence.\n",
    "        self.num_heads = num_heads       # Number of attention heads, Each head learns to focus on different parts of the input.\n",
    "        self.d_k = d_model // num_heads  # Dimnsion of each head. We assume d_v = d_k\n",
    "               \n",
    "        # Linear transformations for queries, keys, and values\n",
    "        # compare queries and keys to compute attention scores, and then use those scores to weight the values\n",
    "        self.W_q = nn.Linear(d_model, d_model) # mapping from a vector of size d_model to another vector of size d_model. This layer transforms the input into a query vector.\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        # Final linear layer to project the concatenated heads' outputs back to d_model dimensions\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        ### WRITE YOUR CODE HERE\n",
    "        \n",
    "        # 1. Calculate attention scores with scaling.\n",
    "        # Computes the similarity (dot product) between queries and keys. q * k^t and scale by sqrt(d_k)\n",
    "        # Q: [batch_size, seq_len, d_k] K: [batch_size, seq_len, d_k]\n",
    "        # Result should be: [batch_size, seq_len, seq_len], Rows = query positions, Columns = key positions\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1))/ math.sqrt(self.d_k) # Transposes the last two dimensions of K (swaps rows and columns)\n",
    "\n",
    "        # 2. Apply mask (if provided) by setting masked positions to a large negative value.\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        # 3. Apply softmax to attention scores to get probabilities.\n",
    "        # Each row sums to 1, creating probability distributions over keys for each query.\n",
    "        attention_weights = torch.softmax(scores, dim=-1) # dim=-1: For each query, softmax across all key positions\n",
    "\n",
    "        # 4. Return the weighted sum of values based on attention probabilities.\n",
    "        # attention_weights: [batch_size, seq_len, seq_len], Represents the attention scores (probabilities) for each query-key pair.\n",
    "        # V (Values): [batch_size, seq_len, d_k], Contains the value vectors for each position in the sequence.\n",
    "        # Output: [batch_size, seq_len, d_k], Each position in the sequence is a weighted sum of the value vectors.\n",
    "        output = torch.matmul(attention_weights, V)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def split_heads(self, x):\n",
    "        # Reshape the input tensor to [batch_size, num_heads, seq_length, d_k]\n",
    "        # to prepare for multi-head attention processing\n",
    "        # input x: [batch_size, seq_length, d_model]; batch_size: Number of sequences in the batch, seq_length: Number of tokens in each sequence, d_model: Total embedding dimension\n",
    "        batch_size, seq_length, d_model = x.size() #Extracts the current tensor dimensions\n",
    "        return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2) # Splits d_model into num_heads Ã— d_k; self.num_heads and self.d_k: Defined in __init__ and stored with the object, vailable throughout the object's lifetime\n",
    "        #After split_heads, each head operates on its own [seq_length, d_k] slice:\n",
    "        # Head 1: x[:, 0, :, :]\n",
    "        # Head 2: x[:, 1, :, :]\n",
    "        # This allows each head to learn different types of relationships in the data.\n",
    "        \n",
    "    def combine_heads(self, x):\n",
    "        # Inverse operation of split_heads: combine the head outputs back into the original tensor shape\n",
    "        # [batch_size, seq_length, d_model]\n",
    "        batch_size, num_heads, seq_length, d_k = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
    "\n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        ### WRITE YOUR CODE HERE\n",
    "        \n",
    "        # 1. Linearly project the queries, keys, and values, and then split them into heads.\n",
    "        #  Input: [batch_size, seq_length, d_model]\n",
    "        # Output: [batch_size, num_heads, seq_length, d_k]\n",
    "        Q = self.split_heads(self.W_q(Q)) # self.W_q = nn.Linear(d_model, d_model), projected_Q = self.W_q(Q), For each position: projected_Q[i] = Q[i] @ W_q + bias_q \n",
    "        K = self.split_heads(self.W_k(K))\n",
    "        V = self.split_heads(self.W_v(V))\n",
    "\n",
    "        # 2. Apply scaled dot-product attention for each head.\n",
    "        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "\n",
    "        # 3. Concatenate the heads' outputs and apply the final linear projection.\n",
    "        output = self.combine_heads(attn_output)\n",
    "        output = self.W_o(output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b13867",
   "metadata": {},
   "source": [
    "### Feed-Forward NN\n",
    "\n",
    "$$\n",
    "    \\text{FFN}(x) = \\max(0, xW_1 + b_1)W_2 + b_2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5ab551",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \"\"\"The Positionwise Feedforward Network (FFN) module\"\"\"\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super().__init__()       \n",
    "        self.linear1 = nn.Linear(d_model, d_ff) # expansion step: Transforms input from [batch, seq_len, d_model] to [batch, seq_len, d_ff]   \n",
    "        self.linear2 = nn.Linear(d_ff, d_model) # Transforms back from [batch, seq_len, d_ff] to [batch, seq_len, d_model],  contraction step       \n",
    "        self.dropout = nn.Dropout(dropout)# Randomly sets some elements to zero during training, Helps prevent overfitting        \n",
    "        self.relu = nn.ReLU() # ReLu activation: max(0, x), non-linearity\n",
    "\n",
    "    def forward(self, x):\n",
    "        ### WRITE YOUR CODE HERE  \n",
    "        # Apply first linear transformation: d_model -> d_ff\n",
    "        x = self.linear1(x)\n",
    "        # Apply ReLU activation\n",
    "        x = self.relu(x)\n",
    "        # Apply dropout for regularization\n",
    "        x = self.dropout(x)\n",
    "        # Apply second linear transformation: d_ff -> d_model\n",
    "        x = self.linear2(x)\n",
    "\n",
    "        return x\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4034ef0a",
   "metadata": {},
   "source": [
    "### Positional Encoding\n",
    "\n",
    "$$\n",
    "    \\text{PE}(pos, 2i) = \\sin(pos/10000^{2i/d_{\\text{model}}}) \\\\\n",
    "    \\text{PE}(pos, 2i + 1) = \\cos(pos/10000^{2i/d_{\\text{model}}})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfca835c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):    \n",
    "    \"\"\"\n",
    "    Implements the positional encoding module using sinusoidal functions of different frequencies \n",
    "    for each dimension of the encoding.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, max_seq_length):\n",
    "        super().__init__()        \n",
    "        \n",
    "        # Create a positional encoding (PE) matrix with dimensions [max_seq_length, d_model].\n",
    "        # This matrix will contain the positional encodings for all possible positions up to max_seq_length.\n",
    "        pe = torch.zeros(max_seq_length, d_model)\n",
    "        \n",
    "        # Generate a tensor of positions (0 to max_seq_length - 1) and reshape it to [max_seq_length, 1].\n",
    "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
    "        \n",
    "        # Compute the division term used in the formulas for sin and cos functions.\n",
    "        # This term is based on the dimension of the model and the position, ensuring that the wavelengths\n",
    "        # form a geometric progression from 2Ï€ to 10000 * 2Ï€. It uses only even indices for the dimensions.\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        \n",
    "        # Apply the sin function to even indices in the PE matrix. These values are determined by\n",
    "        # multiplying the position by the division term, creating a pattern where each position has\n",
    "        # a unique sinusoidal encoding.       \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        \n",
    "        # Apply the cos function to odd indices in the PE matrix, complementing the sin-encoded positions.\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        # Register 'pe' as a buffer within the module. Unlike parameters, buffers are not updated during training.\n",
    "        # This is crucial because positional encodings are fixed and not subject to training updates.\n",
    "        # The unsqueeze(0) adds a batch dimension for easier broadcasting with input tensors.\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Add positional encoding to the input tensor x.\n",
    "        # x is expected to have dimensions [batch_size, seq_length, d_model].\n",
    "        # The positional encoding 'pe' is sliced to match the seq_length of 'x', and then added to 'x'.\n",
    "        # This operation leverages broadcasting to apply the same positional encoding across the batch.\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e2475b",
   "metadata": {},
   "source": [
    "### Encoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000313c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    \"\"\"An encoder layer consists of a multi-head self-attention sublayer and a feed forward sublayer,\n",
    "       with a dropout, residual connection, and layer normalization after each sub-layer.    \n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "        self.layer_norm1 = nn.LayerNorm(d_model)\n",
    "        self.layer_norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        ### WRITE YOUR CODE HERE   \n",
    "        # First sublayer: Multi-head self-attention with residual connection and layer norm\n",
    "        attn_output = self.self_attn(x,x,x,mask) # Self-attention: Q=K=V=x\n",
    "        x = self.layer_norm1(x + self.dropout(attn_output))\n",
    "\n",
    "        # Second sublayer: Feed-forward network with residual connection and layer norm\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.layer_norm2(x +self.dropout(ff_output)) \n",
    "\n",
    "        return x\n",
    "        \n",
    "        \n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76215e02",
   "metadata": {},
   "source": [
    "### Decoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee27007e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    \"\"\"A decoder layer consists of a multi-head self-attention, cross-attention and a feed-forward sublayers,\n",
    "       with a dropout, residual connection, and layer normalization after each sub-layer.    \n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "        self.layer_norm1 = nn.LayerNorm(d_model)\n",
    "        self.layer_norm2 = nn.LayerNorm(d_model)\n",
    "        self.layer_norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)       \n",
    "\n",
    "    def forward(self, x, enc_output, src_mask, tgt_mask):\n",
    "        ### WRITE YOUR CODE HERE\n",
    "        # First sublayer: Masked multi-head self-attention with residual connection and layer norm\n",
    "        self_attn_output = self.self_attn(x,x,x,tgt_mask) # Self-attention process target sequence\n",
    "        x = self.layer_norm1(x + self.dropout(self_attn_output)) \n",
    "\n",
    "        # Second sublayer: Cross-attention with encoder output\n",
    "        # Q: From decoder (x) - \"what am I looking for?\"\n",
    "        # K, V: From encoder output - \"what information is available?\"\n",
    "        cross_attn_output = self.cross_attn(x, enc_output, enc_output, src_mask) # Q=x, K=V=enc_output, Attend to source sequence\n",
    "        x = self.layer_norm2(x + self.dropout(cross_attn_output))\n",
    "\n",
    "        # Third sublayer: Feed-forward with residual connection and layer norm\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.layer_norm3(x + self.dropout(ff_output))\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad86614",
   "metadata": {},
   "source": [
    "### The Full Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31af1f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements the Transformer model for sequence-to-sequence tasks such as machine translation.\n",
    "    The Transformer model, as described in \"Attention is All You Need\" by Vaswani et al., consists of an encoder and\n",
    "    decoder architecture that uses self-attention mechanisms to process input sequences and generate output sequences.\n",
    "\n",
    "    Parameters:\n",
    "    - src_vocab_size (int): Size of the source vocabulary.\n",
    "    - tgt_vocab_size (int): Size of the target vocabulary.\n",
    "    - d_model (int): Dimension of the model embeddings and hidden states.\n",
    "    - N (int): Number of layers in both the encoder and decoder stacks.\n",
    "    - n_heads (int): Number of attention heads in each multi-head attention mechanism.\n",
    "    - d_ff (int): Dimension of the feed-forward network within each layer.\n",
    "    - max_seq_length (int): Maximum length of input sequences, used for positional encoding.\n",
    "    - dropout (float): Dropout rate applied to embeddings and sub-layers.\n",
    "    - pad_idx (int): Index of the padding token in the source and target vocabularies.\n",
    "\n",
    "    Attributes:\n",
    "    - src_embedding (torch.nn.Embedding): Embedding layer for source sequences.\n",
    "    - tgt_embedding (torch.nn.Embedding): Embedding layer for target sequences.\n",
    "    - positional_encoding (PositionalEncoding): Adds positional information to embeddings.\n",
    "    - encoder (torch.nn.ModuleList): Stack of N encoder layers.\n",
    "    - decoder (torch.nn.ModuleList): Stack of N decoder layers.\n",
    "    - out (torch.nn.Linear): Linear layer that projects decoder output to target vocabulary size.\n",
    "    - dropout (torch.nn.Dropout): Dropout layer applied after embedding and positional encoding.\n",
    "    \n",
    "    Methods:\n",
    "    - init_weights: Initializes model parameters using Glorot uniform initialization.\n",
    "    - create_source_mask: Creates a mask for padding tokens in the source sequence to ignore them in attention computations.\n",
    "    - create_target_mask: Creates combined padding and future token masks for the target sequence to prevent attending to future tokens and padding tokens.\n",
    "    - encode: Processes the source sequence through the encoder stack and generates memory states.\n",
    "    - decode: Processes the target sequence through the decoder stack using memory states from the encoder and applicable masks.\n",
    "    - forward: Defines the forward pass of the model using the encode and decode methods.\n",
    "    \"\"\"    \n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, N, n_heads, d_ff, max_seq_length, dropout, pad_idx):\n",
    "        super().__init__()\n",
    "\n",
    "        # Embedding layers for source and target\n",
    "        self.src_embedding = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.tgt_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
    "\n",
    "        # Positional encoding\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\n",
    "\n",
    "        # Encoder and Decoder stacks\n",
    "        self.encoder = nn.ModuleList([EncoderLayer(d_model, n_heads, d_ff, dropout) for _ in range(N)])\n",
    "        self.decoder = nn.ModuleList([DecoderLayer(d_model, n_heads, d_ff, dropout) for _ in range(N)])\n",
    "\n",
    "        # Output linear layer\n",
    "        self.out = nn.Linear(d_model, tgt_vocab_size)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Initialization\n",
    "        self.init_weights()\n",
    "        self.pad_idx = pad_idx\n",
    "\n",
    "    def init_weights(self):\n",
    "        \"\"\"Initialize parameters with Glorot / fan_avg\"\"\"\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "                \n",
    "    def create_source_mask(self, src):\n",
    "        \"\"\"Create a mask for padding tokens in the source\"\"\"            \n",
    "        src_mask = (src != self.pad_idx).unsqueeze(1).unsqueeze(2)  # [batch_size, 1, 1, src_len]\n",
    "        # unsqueeze(1) adds a dimension for the heads of the multi-head attention\n",
    "        # unsqueeze(2) adds a dimension for the attention scores \n",
    "        # This mask can be broadcasted across the src_len dimension of the attention scores, \n",
    "        # effectively masking out specific tokens across all heads and all positions in the sequence. \n",
    "        return src_mask    \n",
    "    \n",
    "    def create_target_mask(self, tgt):\n",
    "        \"\"\"Create masks for both padding tokens and future tokens\"\"\"   \n",
    "        # Target padding mask\n",
    "        tgt_pad_mask = (tgt != self.pad_idx).unsqueeze(1).unsqueeze(3)  # [batch_size, 1, tgt_len, 1]\n",
    "        # unsqueeze(1) adds a dimension for the heads of the multi-head attention\n",
    "        # unsqueeze(3) adds a dimension for the attention scores\n",
    "        # The final shape allows the mask to be broadcast across the attention scores, ensuring positions only \n",
    "        # attend to allowed positions as dictated by the no-peak mask (the preceding positions) and the padding mask.\n",
    "                \n",
    "        # Target no-peak mask\n",
    "        tgt_len = tgt.size(1)        \n",
    "        tgt_nopeak_mask = torch.tril(torch.ones(tgt_len, tgt_len, device=device)).bool()\n",
    "        \n",
    "        # Combine masks\n",
    "        tgt_mask = tgt_pad_mask & tgt_nopeak_mask  # [batch_size, 1, tgt_len, tgt_len]        \n",
    "        return tgt_mask \n",
    "        \n",
    "    def encode(self, src):\n",
    "        \"\"\"Encodes the source sequence using the Transformer encoder stack.\n",
    "        \"\"\"       \n",
    "        src_mask = self.create_source_mask(src)\n",
    "        src = self.dropout(self.positional_encoding(self.src_embedding(src)))\n",
    "        \n",
    "        # Pass through each layer in the encoder        \n",
    "        for layer in self.encoder:\n",
    "            src = layer(src, src_mask)\n",
    "        return src, src_mask\n",
    "        \n",
    "    def decode(self, tgt, memory, src_mask):\n",
    "        \"\"\"Decodes the target sequence using the Transformer decoder stack, given the memory from the encoder.\n",
    "        \"\"\"\n",
    "        tgt_mask = self.create_target_mask(tgt)\n",
    "        tgt = self.dropout(self.positional_encoding(self.tgt_embedding(tgt)))\n",
    "        \n",
    "        # Pass through each layer in the decoder\n",
    "        for layer in self.decoder:\n",
    "            tgt = layer(tgt, memory, src_mask, tgt_mask)\n",
    "\n",
    "        # Output layer\n",
    "        output = self.out(tgt)\n",
    "        return output\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        ### WRITE YOUR CODE HERE\n",
    "        # Encode the source sequence\n",
    "        memory, src_mask = self.encode(src)\n",
    "\n",
    "        # Decode the target sequence using encoder memory\n",
    "        output = self.decode(tgt, memory, src_mask)\n",
    "        \n",
    "    \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a3b60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameters of the model\n",
    "src_vocab_size = 5000  # Size of source vocabulary\n",
    "tgt_vocab_size = 5000  # Size of target vocabulary\n",
    "d_model = 512          # Embedding dimension\n",
    "N = 6                  # Number of encoder and decoder layers\n",
    "num_heads = 8          # Number of attention heads\n",
    "d_ff = 2048            # Dimension of feed forward networks\n",
    "max_seq_length = 100   # Maximum sequence length\n",
    "dropout = 0.1          # Dropout rate\n",
    "pad_idx = 0            # Index of the padding token\n",
    "\n",
    "model = Transformer(src_vocab_size, tgt_vocab_size, d_model, N, num_heads, d_ff, max_seq_length, dropout, pad_idx)\n",
    "\n",
    "# Move the model to the appropriate device (GPU or CPU)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050f7248",
   "metadata": {},
   "source": [
    "### Testing on Random Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149dadd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate random sample data\n",
    "torch.manual_seed(42)\n",
    "\n",
    "src_data = torch.randint(1, src_vocab_size, (64, max_seq_length)).to(device)  # (batch_size, seq_length)\n",
    "tgt_data = torch.randint(1, tgt_vocab_size, (64, max_seq_length)).to(device)  # (batch_size, seq_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5b1c56",
   "metadata": {},
   "source": [
    "#### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45583975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the next token using the first token in the first target tensor\n",
    "model.eval()\n",
    "\n",
    "memory, src_mask = model.encode(src_data[:1, :])\n",
    "output = model.decode(tgt_data[:1, :1], memory, src_mask)\n",
    "y = output.view(-1, tgt_vocab_size).argmax(-1)  \n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b7e1c5",
   "metadata": {},
   "source": [
    "If your code is correct, you should get tensor([990])."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a314148e",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afd8ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model for 10 epochs\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005, betas=(0.9, 0.98), eps=1e-9)\n",
    "grad_clip = 1\n",
    "n_epochs = 10\n",
    "\n",
    "model.train()\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    output = model(src_data, tgt_data[:, :-1])\n",
    "    \n",
    "    # tgt_data is of shape [batch_size, tgt_len]\n",
    "    # output is of shape [batch_size, tgt_len, tgt_vocab_size]\n",
    "    output = output.contiguous().view(-1, tgt_vocab_size)\n",
    "    tgt = tgt_data[:, 1:].contiguous().view(-1)\n",
    "    loss = criterion(output, tgt)        \n",
    "    \n",
    "    loss.backward()        \n",
    "    nn.utils.clip_grad_norm_(model.parameters(), grad_clip)        \n",
    "    optimizer.step()    \n",
    "    print(f'Epoch: {epoch + 1}, Loss: {loss.item()}')   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f36f48",
   "metadata": {},
   "source": [
    "You should see the loss decreasing from around 8.6 to 8.1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71caa7ed",
   "metadata": {},
   "source": [
    "### Machine Translation Example\n",
    "\n",
    "We now consider a real-world example using the Multi30k German-English translation task. This task is much smaller than the WMT task considered in the paper (only 30K sentence pairs compared to 4.5M pairs in the WMT-14 English-German dataset), but it illustrates the whole system. <br>\n",
    "It is recommended to run this example on Google Colab, or on a machine with a strong GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6ecd8c",
   "metadata": {},
   "source": [
    "#### Define Tokenizers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f20abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load spacy models for tokenization\n",
    "try:\n",
    "    spacy_de = spacy.load('de_core_news_sm')\n",
    "except IOError:\n",
    "    os.system(\"python -m spacy download de_core_news_sm\")\n",
    "    spacy_de = spacy.load('de_core_news_sm')\n",
    "\n",
    "try:\n",
    "    spacy_en = spacy.load('en_core_web_sm')\n",
    "except IOError:\n",
    "    os.system(\"python -m spacy download en_core_web_sm\")\n",
    "    spacy_en = spacy.load('en_core_web_sm')\n",
    "\n",
    "def tokenize_de(text):\n",
    "    return [tok.text for tok in spacy_de.tokenizer(text)]\n",
    "\n",
    "def tokenize_en(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "def yield_tokens(data_iter, tokenizer, language):\n",
    "    for data_sample in data_iter:\n",
    "        yield tokenizer(data_sample[language])\n",
    "\n",
    "tokenizer_de = get_tokenizer(tokenize_de)\n",
    "tokenizer_en = get_tokenizer(tokenize_en)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e909ce2a",
   "metadata": {},
   "source": [
    "#### Build Vocabularies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed8dff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, _, _ = Multi30k(split=('train', 'valid', 'test'))\n",
    "vocab_src = build_vocab_from_iterator(yield_tokens(train_data, tokenizer_de, 0), \n",
    "                                      specials=['<unk>', '<pad>', '<bos>', '<eos>'])\n",
    "vocab_tgt = build_vocab_from_iterator(yield_tokens(train_data, tokenizer_en, 1), \n",
    "                                      specials=['<unk>', '<pad>', '<bos>', '<eos>'])\n",
    "\n",
    "vocab_src.set_default_index(vocab_src['<unk>'])\n",
    "vocab_tgt.set_default_index(vocab_tgt['<unk>'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936560a1",
   "metadata": {},
   "source": [
    "#### Create the Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c862170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameters of the model\n",
    "src_vocab_size = len(vocab_src)  # Size of source vocabulary\n",
    "tgt_vocab_size = len(vocab_tgt)  # Size of target vocabulary\n",
    "d_model = 512  # Embedding dimension\n",
    "N = 6          # Number of encoder and decoder layers\n",
    "num_heads = 8  # Number of attention heads\n",
    "d_ff = 2048    # Dimension of feed forward networks\n",
    "max_seq_length = 5000 # Maximum sequence length\n",
    "dropout = 0.1  # Dropout rate\n",
    "\n",
    "# Assume pad_idx is the padding index in the target vocabulary\n",
    "pad_idx = vocab_tgt['<pad>']\n",
    "\n",
    "# Initialize the Transformer model\n",
    "model = Transformer(src_vocab_size, tgt_vocab_size, d_model, N, num_heads, d_ff, max_seq_length, dropout, pad_idx)\n",
    "\n",
    "# Move the model to the appropriate device (GPU or CPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "# Hyperparameters for the training process\n",
    "batch_size = 128\n",
    "grad_clip = 1\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "# Initialize the loss function with CrossEntropyLoss, ignoring the padding index\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d3b94d",
   "metadata": {},
   "source": [
    "#### Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca10d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_process(raw_data_iter):\n",
    "    data = []\n",
    "    for raw_src, raw_tgt in raw_data_iter:\n",
    "        src_tensor = torch.tensor([vocab_src[token] for token in tokenizer_de(raw_src)], dtype=torch.long)\n",
    "        tgt_tensor = torch.tensor([vocab_tgt[token] for token in tokenizer_en(raw_tgt)], dtype=torch.long)\n",
    "        data.append((src_tensor, tgt_tensor))\n",
    "    return data\n",
    "\n",
    "train_data, valid_data, test_data = Multi30k(split=('train', 'valid', 'test'))\n",
    "train_data = data_process(train_data)\n",
    "valid_data = data_process(valid_data)\n",
    "#test_data = data_process(test_data)   \n",
    "# The test set of Multi30k is corrupted\n",
    "# See https://discuss.pytorch.org/t/unicodedecodeerror-when-running-test-iterator/192818/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2f452a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(data_batch):\n",
    "    \"\"\"Processes a batch of source-target pairs by adding start-of-sequence (BOS) and end-of-sequence (EOS) tokens\n",
    "    to each sequence and padding all sequences to the same length.\n",
    "    \n",
    "    Parameters:\n",
    "    - data_batch (Iterable[Tuple[Tensor, Tensor]]): A batch of source-target pairs, where each element is a tuple\n",
    "      containing the source sequence tensor and the target sequence tensor.\n",
    "    \"\"\"\n",
    "    src_batch, tgt_batch = [], []\n",
    "    src_batch, tgt_batch = [], []\n",
    "    \n",
    "    # Iterate over each source-target pair in the provided batch\n",
    "    for src_item, tgt_item in data_batch:\n",
    "        # Prepend the start-of-sequence (BOS) token and append the end-of-sequence (EOS) token to the sequences        \n",
    "        src_batch.append(torch.cat([torch.tensor([vocab_src['<bos>']]), src_item, \n",
    "                                    torch.tensor([vocab_src['<eos>']])], dim=0))\n",
    "        tgt_batch.append(torch.cat([torch.tensor([vocab_tgt['<bos>']]), tgt_item, \n",
    "                                    torch.tensor([vocab_tgt['<eos>']])], dim=0))\n",
    "        \n",
    "    # Pad the sequences in the source batch to ensure they all have the same length.\n",
    "    # 'batch_first=True' indicates that the batch dimension should come first in the resulting tensor.\n",
    "    src_batch = pad_sequence(src_batch, padding_value=vocab_src['<pad>'], batch_first=True)\n",
    "    tgt_batch = pad_sequence(tgt_batch, padding_value=vocab_tgt['<pad>'], batch_first=True)\n",
    "    return src_batch, tgt_batch\n",
    "\n",
    "# DataLoader for the training data, using the generate_batch function as the collate_fn.\n",
    "# This allows custom processing of each batch (adding BOS/EOS tokens and padding) before being fed into the model.\n",
    "train_iterator = DataLoader(train_data, batch_size=batch_size, shuffle=True, collate_fn=generate_batch)\n",
    "\n",
    "# Similarly, DataLoader for the validation data\n",
    "valid_iterator = DataLoader(valid_data, batch_size=batch_size, shuffle=True, collate_fn=generate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbd8431",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, grad_clip):\n",
    "    \"\"\"\n",
    "    Trains the model for one epoch over the given dataset.\n",
    "    This function iterates over the provided data iterator, performing the forward and backward passes for each batch.\n",
    "    It employs teacher forcing by feeding the shifted target sequence (excluding the last token) as input to the decoder.\n",
    "    \n",
    "    Parameters:\n",
    "    - model (torch.nn.Module): The model to be trained. \n",
    "    - iterator (Iterable): An iterable object that returns batches of data. \n",
    "    - optimizer (torch.optim.Optimizer): The optimizer to use for updating the model parameters.\n",
    "    - criterion (Callable): The loss function used to compute the difference between the model's predictions and the actual targets.\n",
    "    - grad_clip (float): The maximum norm of the gradients for gradient clipping. \n",
    "\n",
    "    Returns:\n",
    "    - float: The average loss for the epoch, computed as the total loss over all batches divided by the number of batches in the iterator.\n",
    "    \"\"\"    \n",
    "    # Set the model to training mode. \n",
    "    # This enables dropout, layer normalization etc., which behave differently during training.\n",
    "    model.train()   \n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    # Enumerate over the data iterator to get batches\n",
    "    for i, batch in enumerate(iterator):         \n",
    "        # Unpack the batch to get source (src) and target (tgt) sequences\n",
    "        src, tgt = batch\n",
    "        src, tgt = src.to(device), tgt.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass through the model. \n",
    "        # For seq2seq models, the decoder input (tgt[:, :-1]) excludes the last token, implementing teacher forcing.\n",
    "        output = model(src, tgt[:, :-1])\n",
    "        \n",
    "        # Reshape the output and target tensors to compute loss.\n",
    "        # The output tensor is reshaped to a 2D tensor where rows correspond to each token in the batch and columns to vocabulary size.\n",
    "                \n",
    "        # tgt is of shape [batch_size, tgt_len]\n",
    "        # output is of shape [batch_size, tgt_len, tgt_vocab_size]\n",
    "        output = output.contiguous().view(-1, tgt_vocab_size)\n",
    "        \n",
    "        # The target tensor is reshaped to a 1D tensor, excluding the first token (BOS) from each sequence.\n",
    "        tgt = tgt[:, 1:].contiguous().view(-1)\n",
    "        \n",
    "        # Compute loss, perform backpropagation, and update model parameters\n",
    "        loss = criterion(output, tgt)          \n",
    "        loss.backward() \n",
    "        nn.utils.clip_grad_norm_(model.parameters(), grad_clip)  \n",
    "        optimizer.step()        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    # Compute average loss per batch for the current epoch\n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d18844",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \"\"\"\n",
    "    Evaluates the model's performance on a given dataset.\n",
    "    This function is similar to the training loop, but without the backward pass and parameter updates.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(iterator):\n",
    "            src, tgt = batch\n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "            output = model(src, tgt[:, :-1])            \n",
    "            output_dim = output.shape[-1]            \n",
    "            output = output.contiguous().view(-1, output_dim)\n",
    "            tgt = tgt[:, 1:].contiguous().view(-1)\n",
    "            loss = criterion(output, tgt)\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba53f24",
   "metadata": {},
   "source": [
    "#### Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02463b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 20\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss = train(model, train_iterator, optimizer, criterion, grad_clip)\n",
    "    val_loss = evaluate(model, valid_iterator, criterion)\n",
    "\n",
    "    print(f'\\nEpoch: {epoch + 1}')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f}')\n",
    "    print(f'\\tVal Loss: {val_loss:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f39e45",
   "metadata": {},
   "source": [
    "The train loss should decrease from around 5.7 to 2.8 after 20 epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7d30f7",
   "metadata": {},
   "source": [
    "#### Translating a Sample Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15293ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence(model, sentence, vocab_src, vocab_tgt, max_length=50):\n",
    "    \"\"\"\n",
    "    Translates a given source sentence into the target language using a trained Transformer model.\n",
    "    The function preprocesses the input sentence by tokenizing and converting it to tensor format, then uses the model's\n",
    "    encode and decode methods to generate the translated sentence. The translation process is performed token by token\n",
    "    using greedy decoding, selecting the most likely next token at each step until an <eos> token is produced or the\n",
    "    maximum length is reached.\n",
    "\n",
    "    Parameters:\n",
    "    - model (torch.nn.Module): The trained Transformer model. \n",
    "    - sentence (str): The source sentence to translate. \n",
    "    - vocab_src (dict): The source vocabulary mapping of tokens to indices. It should include special tokens such as\n",
    "      '<bos>' (beginning of sentence) and '<eos>' (end of sentence).\n",
    "    - vocab_tgt (dict): The target vocabulary mapping of indices to tokens. It should provide a method `lookup_token`\n",
    "      to convert token indices back to the string representation.\n",
    "    - max_length (int, optional): The maximum allowed length for the generated translation. The decoding process will\n",
    "      stop when this length is reached if an <eos> token has not yet been generated.\n",
    "\n",
    "    Returns:\n",
    "    - str: The translated sentence as a string of text in the target language.\n",
    "    \"\"\" \n",
    "    ### WRITE YOUR CODE HERE\n",
    "    model.eval()\n",
    "\n",
    "    # Tokenize and convert source sentence to tensor\n",
    "    src_tokens = [vocab_src['<bos>']] + [vocab_src[token] for token in tokenize_de(sentence)] + [vocab_src['<eos']]\n",
    "    src_tensor = torch.tensor(src_tokens, dtype = torch.long).unsqueeze(0).to(device) # add batch dimension\n",
    "    \n",
    "    # Encode the source sequence\n",
    "    memory, src_mask = model.encode(src_tensor)\n",
    "\n",
    "    # Initialize target sequence with <bos> token\n",
    "    tgt_tokens = [vocab_tgt['<bos>']]\n",
    "\n",
    "    # Generate translation token by token (greedy decoding)\n",
    "    for _ in range(max_length):\n",
    "        # Convert current target tokens to tensor\n",
    "        tgt_tensor = torch.tensor(tgt_tokens, dtype=torch.long).unsqueeze(0).to(device)\n",
    "        # Decode to get next token probabilities\n",
    "        output = model.decode(tgt_tensor, memory, src_mask)\n",
    "        # Get the most likely next token (greedy selection)\n",
    "        next_token = output[:, -1, :].argmax(-1).item() # Get last position, most likely token\n",
    "        # Add the predicted token to the sequence\n",
    "        tgt_tokens.append(next_token)\n",
    "        # Stop if we generate <eos> token\n",
    "        if next_token == vocab_tgt['<eos>']:\n",
    "            break\n",
    "    # Convert token indices back to words (excluding <bos> and <eos>)\n",
    "    translated_tokens = [vocab_tgt.lookup_token(token) for token in tgt_tokens[1:-1]] # Skip <bos> and <eos>; .lookup_token(): A method that converts token indices back to strings\n",
    "    # Join tokens into a sentence\n",
    "    translated_sentence = ' '.join(translated_tokens)\n",
    "    \n",
    "    \n",
    "    return translated_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baeb62e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_sentence = \"Ein kleiner Junge spielt drauÃŸen mit einem Ball.\"  # German for \"A little boy playing outside with a ball.\"\n",
    "translated_sentence = translate_sentence(model, src_sentence, vocab_src, vocab_tgt)\n",
    "print(f'Translated sentence: {translated_sentence}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26642c2e",
   "metadata": {},
   "source": [
    "You should get a translation similar to the reference after 20 epochs of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8feeea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# hyperparameter ranges to experiment with\n",
    "hyperparameter_configs = {\n",
    "    'num_heads': [4, 8, 16],\n",
    "    'num_layers': [3, 6, 9],\n",
    "    'learning_rate': [0.0001, 0.0005, 0.001],\n",
    "    'batch_size': [32, 64, 128]\n",
    "}\n",
    "\n",
    "# Generate all combinations \n",
    "def generate_configs(configs, max_experiments=20):\n",
    "    \"\"\"Generate hyperparameter combinations\"\"\"\n",
    "    keys = list(configs.keys())\n",
    "    values = list(configs.values())\n",
    "    \n",
    "    all_combinations = list(itertools.product(*values))\n",
    "    \n",
    "    # Limit number of experiments if too many\n",
    "    if len(all_combinations) > max_experiments:\n",
    "        # Select random subset or use grid search strategy\n",
    "        import random\n",
    "        random.seed(42)\n",
    "        all_combinations = random.sample(all_combinations, max_experiments)\n",
    "    \n",
    "    return [dict(zip(keys, combo)) for combo in all_combinations]\n",
    "\n",
    "experiments = generate_configs(hyperparameter_configs, max_experiments=3)\n",
    "print(f\"Total experiments: {len(experiments)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a29d1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_with_config(config, src_vocab_size, tgt_vocab_size, pad_idx):\n",
    "    \"\"\"Create model with specific hyperparameters\"\"\"\n",
    "    \n",
    "    # Fixed hyperparameters\n",
    "    d_model = 512\n",
    "    d_ff = 2048\n",
    "    max_seq_length = 5000\n",
    "    dropout = 0.1\n",
    "    \n",
    "    model = Transformer(\n",
    "        src_vocab_size=src_vocab_size,\n",
    "        tgt_vocab_size=tgt_vocab_size,\n",
    "        d_model=d_model,\n",
    "        N=config['num_layers'],           # Variable\n",
    "        n_heads=config['num_heads'],    # Variable\n",
    "        d_ff=d_ff,\n",
    "        max_seq_length=max_seq_length,\n",
    "        dropout=dropout,\n",
    "        pad_idx=pad_idx\n",
    "    )\n",
    "    \n",
    "    return model.to(device)\n",
    "\n",
    "def create_optimizer_with_config(model, config):\n",
    "    \"\"\"Create optimizer with specific learning rate\"\"\"\n",
    "    return optim.Adam(\n",
    "        model.parameters(), \n",
    "        lr=config['learning_rate'], \n",
    "        betas=(0.9, 0.98), \n",
    "        eps=1e-9\n",
    "    )\n",
    "\n",
    "def create_dataloader_with_config(data, config, collate_fn):\n",
    "    \"\"\"Create dataloader with specific batch size\"\"\"\n",
    "    return DataLoader(\n",
    "        data, \n",
    "        batch_size=config['batch_size'], \n",
    "        shuffle=True, \n",
    "        collate_fn=collate_fn\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31a8e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_metrics(model, train_iterator, valid_iterator, optimizer, criterion, config, num_epochs=3):\n",
    "    \"\"\"Train model and collect metrics\"\"\"\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        epoch_train_loss = 0\n",
    "        \n",
    "        for batch in train_iterator:\n",
    "            src, tgt = batch\n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model(src, tgt[:, :-1])\n",
    "            \n",
    "            output = output.contiguous().view(-1, tgt_vocab_size)\n",
    "            target = tgt[:, 1:].contiguous().view(-1)\n",
    "            \n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_train_loss += loss.item()\n",
    "        \n",
    "        avg_train_loss = epoch_train_loss / len(train_iterator)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        epoch_val_loss = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in valid_iterator:\n",
    "                src, tgt = batch\n",
    "                src, tgt = src.to(device), tgt.to(device)\n",
    "                \n",
    "                output = model(src, tgt[:, :-1])\n",
    "                output = output.contiguous().view(-1, tgt_vocab_size)\n",
    "                target = tgt[:, 1:].contiguous().view(-1)\n",
    "                \n",
    "                loss = criterion(output, target)\n",
    "                epoch_val_loss += loss.item()\n",
    "        \n",
    "        avg_val_loss = epoch_val_loss / len(valid_iterator)\n",
    "        \n",
    "        train_losses.append(avg_train_loss)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        \n",
    "        # Track best validation loss\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{num_epochs} - Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}')\n",
    "    \n",
    "    return {\n",
    "        'config': config,\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "        'best_val_loss': best_val_loss,\n",
    "        'final_train_loss': train_losses[-1],\n",
    "        'final_val_loss': val_losses[-1]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be91e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiments(experiments, train_data, valid_data, vocab_src, vocab_tgt):\n",
    "    \"\"\"Run all hyperparameter experiments\"\"\"\n",
    "    \n",
    "    results = []\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=vocab_tgt['<pad>'])\n",
    "    \n",
    "    for i, config in enumerate(experiments):\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Experiment {i+1}/{len(experiments)}\")\n",
    "        print(f\"Config: {config}\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        try:\n",
    "            # Create model\n",
    "            model = create_model_with_config(config, len(vocab_src), len(vocab_tgt), vocab_tgt['<pad>'])\n",
    "            \n",
    "            # Create optimizer\n",
    "            optimizer = create_optimizer_with_config(model, config)\n",
    "            \n",
    "            # Create data loaders\n",
    "            train_iterator = create_dataloader_with_config(train_data, config, generate_batch)\n",
    "            valid_iterator = create_dataloader_with_config(valid_data, config, generate_batch)\n",
    "            \n",
    "            # Train and collect results\n",
    "            result = train_with_metrics(\n",
    "                model, train_iterator, valid_iterator, \n",
    "                optimizer, criterion, config, num_epochs=3\n",
    "            )\n",
    "            \n",
    "            results.append(result)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Experiment {i+1} failed: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run experiments\n",
    "experiment_results = run_experiments(experiments, train_data, valid_data, vocab_src, vocab_tgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a3418e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_results(results):\n",
    "    \"\"\"Analyze experimental results\"\"\"\n",
    "    \n",
    "    # Convert to DataFrame for easier analysis\n",
    "    data = []\n",
    "    for result in results:\n",
    "        config = result['config']\n",
    "        data.append({\n",
    "            'num_heads': config['num_heads'],\n",
    "            'num_layers': config['num_layers'], \n",
    "            'learning_rate': config['learning_rate'],\n",
    "            'batch_size': config['batch_size'],\n",
    "            'best_val_loss': result['best_val_loss'],\n",
    "            'final_val_loss': result['final_val_loss'],\n",
    "            'final_train_loss': result['final_train_loss']\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "# Analyze results\n",
    "results_df = analyze_results(experiment_results)\n",
    "\n",
    "# Sort by best validation loss\n",
    "results_df_sorted = results_df.sort_values('best_val_loss')\n",
    "print(\"Top 5 configurations by validation loss:\")\n",
    "print(results_df_sorted.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9bdaed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def statistical_analysis(results_df):\n",
    "    \"\"\"Perform statistical analysis of hyperparameter effects\"\"\"\n",
    "    \n",
    "    print(\"Correlation Analysis:\")\n",
    "    print(\"=\"*50)\n",
    "    correlations = results_df[['num_heads', 'num_layers', 'learning_rate', 'batch_size', 'best_val_loss']].corr()\n",
    "    print(correlations['best_val_loss'].sort_values())\n",
    "    \n",
    "    print(\"\\nBest hyperparameter values:\")\n",
    "    print(\"=\"*50)\n",
    "    best_config = results_df.loc[results_df['best_val_loss'].idxmin()]\n",
    "    print(f\"Best validation loss: {best_config['best_val_loss']:.4f}\")\n",
    "    print(f\"Configuration:\")\n",
    "    print(f\"  - Number of heads: {best_config['num_heads']}\")\n",
    "    print(f\"  - Number of layers: {best_config['num_layers']}\")\n",
    "    print(f\"  - Learning rate: {best_config['learning_rate']}\")\n",
    "    print(f\"  - Batch size: {best_config['batch_size']}\")\n",
    "    \n",
    "    return best_config\n",
    "\n",
    "best_config = statistical_analysis(results_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
