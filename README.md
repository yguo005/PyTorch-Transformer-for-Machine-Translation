# PyTorch Transformer for Machine Translation
# Transformer Architecture: PyTorch Implementation

This project is a complete implementation of the Transformer architecture from the seminal paper _"Attention Is All You Need"_ by Vaswani et al. The model is built from scratch using PyTorch and is trained on the Multi30k dataset for a German-to-English machine translation task.


The project not only builds and trains the model but also includes a systematic framework for hyperparameter experimentation and analysis, allowing for the comparison of different model configurations.


---

## Features

- **Full Transformer Implementation:** Complete Encoder-Decoder architecture built with modular PyTorch `nn.Module` classes.
- **Core Components from Scratch:** Includes implementations of Multi-Head Self-Attention, Position-wise Feed-Forward Networks, and Sinusoidal Positional Encodings.
- **Machine Translation Task:** Trained and evaluated on the Multi30k German-to-English dataset.
- **Greedy Decoding Inference:** A `translate_sentence` function to perform inference on new sentences using a greedy decoding strategy.
- **Systematic Hyperparameter Tuning:** A robust framework to automatically run experiments with different combinations of hyperparameters (`num_heads`, `num_layers`, `learning_rate`, `batch_size`).
- **Performance Analysis:** Uses pandas and matplotlib to analyze and visualize the results of hyperparameter experiments to find the optimal configuration.

---

## Architecture Overview

The model is composed of several key components, each implemented as a separate class for modularity:

- **Multi-Head Attention (`MultiHeadAttention`):** Implements the scaled dot-product attention mechanism, split across multiple heads to allow the model to jointly attend to information from different representation subspaces.
- **Position-wise Feed-Forward Network (`PositionwiseFeedForward`):** A simple, fully connected feed-forward network applied to each position separately and identically.
- **Positional Encoding (`PositionalEncoding`):** Since the model contains no recurrence, positional encodings are added to the input embeddings to give the model information about the relative or absolute position of tokens.
- **Encoder Layer (`EncoderLayer`):** Consists of a multi-head self-attention sublayer and a feed-forward sublayer.
- **Decoder Layer (`DecoderLayer`):** Consists of a masked multi-head self-attention sublayer, a cross-attention sublayer (attending to the encoder's output), and a feed-forward sublayer.
- **Full Transformer (`Transformer`):** Stacks the encoder and decoder layers, manages embeddings, and includes the final linear layer to produce output probabilities. It also handles the creation of padding masks and look-ahead masks.

## Hyperparameter Experimentation

A key feature of this project is the experimental framework designed to evaluate the impact of key hyperparameters on model performance.

### Hyperparameter Space

The framework explores combinations of the following hyperparameters:

- **num_heads:** `[4, 8, 16]`
- **num_layers:** `[3, 6, 9]`
- **learning_rate:** `[0.0001, 0.0005, 0.001]`
- **batch_size:** `[32, 64, 128]`

### Process

1. A list of experimental configurations is generated by taking all combinations of the specified hyperparameters.
2. For each configuration:
   - A new Transformer model, optimizer, and data loaders are created.
   - The model is trained for a set number of epochs (e.g., 3-5 for efficiency).
   - Training and validation losses are recorded for each epoch.
3. Results from all experiments are collected into a pandas DataFrame for analysis.

### Analysis

- The results are analyzed to find the best-performing configuration based on the lowest validation loss.
- The framework also includes correlation analysis to understand the impact of each hyperparameter on model performance.
